{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/home/mlfm/tam/python_graph_utilities_v2/Codes/')\n",
    "import torch\n",
    "from dataset import OneDDatasetBuilder, OneDDatasetLoader\n",
    "from train import train, eval\n",
    "from plot import *\n",
    "from preprocessing import dataset_split_to_loader\n",
    "from networks_lstm import PARC\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define arguments\n",
    "class objectview(object):\n",
    "    def __init__(self, d) -> None:\n",
    "        self.__dict__ = d\n",
    "    def setattr(self, attr_name, attr_value):\n",
    "        self.__dict__[attr_name] = attr_value\n",
    "\n",
    "args = objectview(d={\n",
    "    'total_time': 4.8,\n",
    "    'n_times': 201,\n",
    "    'batch_size': 1000,\n",
    "    'batch_n_times': 50,\n",
    "    'batch_step': 1,\n",
    "    'batch_recursive': True,\n",
    "    'device': torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    'lr': 5e-7,\n",
    "    'weight_decay': 5e-4,\n",
    "    'epoch': 100,\n",
    "    'criterion': torch.nn.MSELoss(),\n",
    "    'n_hiddenfields': 64,\n",
    "    'n_hiddens': 10\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dataset\n",
    "# dataset = OneDDatasetBuilder(\n",
    "#     raw_dir='/data1/tam/datasets',\n",
    "#     root_dir='/data1/tam/downloaded_datasets_new',\n",
    "#     data_names='all',\n",
    "#     time_names=[str(i).zfill(3) for i in range(201)],\n",
    "#     data_type=torch.float32\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load to normalize and batch\n",
    "dataset = OneDDatasetLoader(\n",
    "    root_dir='/data1/tam/downloaded_datasets_new',\n",
    "    sub_dir='normed_and_batched',\n",
    "    data_names='all',\n",
    "    time_names=[str(i).zfill(3) for i in range(201)]\n",
    ")\n",
    "\n",
    "# normalized_dataset = dataset.normalizing(\n",
    "#     sub_dir='normalized',\n",
    "#     scalers = {\n",
    "#         'node_attr' : ['minmax_scaler', 0],\n",
    "#         'edge_attr' : ['quantile_transformer', 0],\n",
    "#         'pressure' : ['quantile_transformer', None],\n",
    "#         'flowrate' : ['quantile_transformer', None]\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# batched_dataset = normalized_dataset.batching(\n",
    "#     batch_size = args.batch_size,\n",
    "#     batch_n_times = args.batch_n_times, \n",
    "#     recursive = args.batch_recursive, \n",
    "#     sub_dir='/normed_and_batched', \n",
    "#     step=args.batch_step\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch_geometric.loader.dataloader.DataLoader at 0x7ff00082be10>,\n",
       " <torch_geometric.loader.dataloader.DataLoader at 0x7ff00027cbd0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Prepare data\n",
    "train_loader, test_loader = dataset_split_to_loader(\n",
    "    dataset = dataset,\n",
    "    subset_ids = {\n",
    "        'train': list(range(0, 10)),\n",
    "        'test': list(range(20, 30))\n",
    "    },\n",
    "    n_datas_per_batch = 6\n",
    ")\n",
    "train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare model\n",
    "_test_data = dataset[0]\n",
    "model = PARC(\n",
    "    n_fields=2,\n",
    "    n_timesteps=_test_data.pressure.size(1),\n",
    "    n_meshfields=(_test_data.node_attr.size(1),_test_data.edge_attr.size(1)), # Tuple(n_node_fields, n_mesh_fields)\n",
    "    n_bcfields=1,\n",
    "    n_hiddenfields=args.n_hiddenfields,\n",
    "    n_hiddens=args.n_hiddens\n",
    ").to(args.device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "args.setattr(attr_name='optimizer', attr_value=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=3, out_features=64, bias=True)\n",
       "  (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=64, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.derivative_solver.node_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 210.00 MiB (GPU 1; 11.77 GiB total capacity; 10.27 GiB already allocated; 200.06 MiB free; 10.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/mlfm/tam/python_graph_utilities_v2/Run/run_train_v1.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B155.230.94.82/home/mlfm/tam/python_graph_utilities_v2/Run/run_train_v1.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(train_loader\u001b[39m.\u001b[39m\u001b[39m__len__\u001b[39m()):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B155.230.94.82/home/mlfm/tam/python_graph_utilities_v2/Run/run_train_v1.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(train_loader))\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B155.230.94.82/home/mlfm/tam/python_graph_utilities_v2/Run/run_train_v1.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m train(model\u001b[39m=\u001b[39;49mmodel, data\u001b[39m=\u001b[39;49mdata, args\u001b[39m=\u001b[39;49margs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B155.230.94.82/home/mlfm/tam/python_graph_utilities_v2/Run/run_train_v1.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m train_loss \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m train_loader\u001b[39m.\u001b[39m\u001b[39m__len__\u001b[39m() \u001b[39m# len(train_dataset)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B155.230.94.82/home/mlfm/tam/python_graph_utilities_v2/Run/run_train_v1.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m total_train_loss\u001b[39m.\u001b[39mappend(train_loss)\n",
      "File \u001b[0;32m~/tam/python_graph_utilities_v2/Codes/train.py:34\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data, args)\u001b[0m\n\u001b[1;32m     31\u001b[0m F_bc \u001b[39m=\u001b[39m F_bc\u001b[39m.\u001b[39mto(args\u001b[39m.\u001b[39mdevice)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m     33\u001b[0m \u001b[39m## Predict output\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m Fs, F_dots \u001b[39m=\u001b[39m model(\n\u001b[1;32m     35\u001b[0m     F_initial\u001b[39m=\u001b[39;49mF_initial, \n\u001b[1;32m     36\u001b[0m     mesh_features\u001b[39m=\u001b[39;49mmesh_features, \n\u001b[1;32m     37\u001b[0m     edge_index\u001b[39m=\u001b[39;49medge_index, \n\u001b[1;32m     38\u001b[0m     F_boundary\u001b[39m=\u001b[39;49mF_bc, \n\u001b[1;32m     39\u001b[0m     timestep\u001b[39m=\u001b[39;49mtimestep\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[39m## Ground truth Fields tensor(pressure, flowrate): Tensor(n_nodes, 1:n_times, n_fields)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m Fs_hat \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([\n\u001b[1;32m     44\u001b[0m     data\u001b[39m.\u001b[39mpressure\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), \n\u001b[1;32m     45\u001b[0m     data\u001b[39m.\u001b[39mflowrate\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     46\u001b[0m ], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(args\u001b[39m.\u001b[39mdevice)\u001b[39m.\u001b[39mfloat() \u001b[39m# concat pressure and flowrate\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/geometric/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/tam/python_graph_utilities_v2/Codes/networks_lstm.py:333\u001b[0m, in \u001b[0;36mPARC.forward\u001b[0;34m(self, F_initial, mesh_features, edge_index, F_boundary, timestep)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_bcfields \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    331\u001b[0m     F_temp \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([F_temp, F_boundary[:, timestep]\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 333\u001b[0m F_dot, _, F_hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mderivative_solver(\n\u001b[1;32m    334\u001b[0m     x \u001b[39m=\u001b[39;49m F_temp, \n\u001b[1;32m    335\u001b[0m     edge_index\u001b[39m=\u001b[39;49medge_index,\n\u001b[1;32m    336\u001b[0m     edge_attr \u001b[39m=\u001b[39;49m _mesh_features,\n\u001b[1;32m    337\u001b[0m     hx \u001b[39m=\u001b[39;49m F_hidden\n\u001b[1;32m    338\u001b[0m )\n\u001b[1;32m    340\u001b[0m \u001b[39m# Numerical scheme\u001b[39;00m\n\u001b[1;32m    341\u001b[0m F_current \u001b[39m=\u001b[39m F_previous \u001b[39m+\u001b[39m timestep \u001b[39m*\u001b[39m F_dot\n",
      "File \u001b[0;32m~/anaconda3/envs/geometric/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/tam/python_graph_utilities_v2/Codes/networks_lstm.py:228\u001b[0m, in \u001b[0;36mLSTMMeshGraphNet.forward\u001b[0;34m(self, x, edge_index, edge_attr, hx, size)\u001b[0m\n\u001b[1;32m    221\u001b[0m     edge_attr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39medge_encoder(edge_attr)\n\u001b[1;32m    223\u001b[0m \u001b[39m# if hidden_edge_features is None:\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[39m#     hidden_edge_features = torch.zeros((edge_attr.size(0), self.hidden_size)).float()\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[39m#     cell_edge_features = torch.zeros((edge_attr.size(0), self.hidden_size)).float()\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[39m# print(hidden_edge_features.size(), cell_edge_features.size())\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m updated_nodes, updated_edges, hx0, hx1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate(\n\u001b[1;32m    229\u001b[0m     edge_index,\n\u001b[1;32m    230\u001b[0m     x \u001b[39m=\u001b[39;49m x,\n\u001b[1;32m    231\u001b[0m     edge_attr \u001b[39m=\u001b[39;49m edge_attr,\n\u001b[1;32m    232\u001b[0m     hx \u001b[39m=\u001b[39;49m hx,\n\u001b[1;32m    233\u001b[0m     size \u001b[39m=\u001b[39;49m size\n\u001b[1;32m    234\u001b[0m )\n\u001b[1;32m    236\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_decoder \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    237\u001b[0m     updated_nodes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_decoder(updated_nodes)\n",
      "File \u001b[0;32m~/anaconda3/envs/geometric/lib/python3.11/site-packages/torch_geometric/nn/conv/message_passing.py:467\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    466\u001b[0m         msg_kwargs \u001b[39m=\u001b[39m res[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(res, \u001b[39mtuple\u001b[39m) \u001b[39melse\u001b[39;00m res\n\u001b[0;32m--> 467\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmessage(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmsg_kwargs)\n\u001b[1;32m    468\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_message_forward_hooks\u001b[39m.\u001b[39mvalues():\n\u001b[1;32m    469\u001b[0m     res \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, (msg_kwargs, ), out)\n",
      "File \u001b[0;32m~/tam/python_graph_utilities_v2/Codes/networks_lstm.py:249\u001b[0m, in \u001b[0;36mLSTMMeshGraphNet.message\u001b[0;34m(self, x_i, x_j, edge_attr, hx)\u001b[0m\n\u001b[1;32m    247\u001b[0m     updated_edges \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([updated_edges, edge_attr], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    248\u001b[0m     \u001b[39m# updated_edges = edge_attr + self.edge_mlp(updated_edges) # skip connection\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m     updated_edges, hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49medge_mlp(\n\u001b[1;32m    250\u001b[0m         \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49mupdated_edges, hx\u001b[39m=\u001b[39;49mhx)\n\u001b[1;32m    251\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m     updated_edges, hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39medge_mlp(\n\u001b[1;32m    253\u001b[0m         \u001b[39minput\u001b[39m\u001b[39m=\u001b[39mupdated_edges, hx\u001b[39m=\u001b[39mhx)\n",
      "File \u001b[0;32m~/anaconda3/envs/geometric/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/geometric/lib/python3.11/site-packages/torch/nn/modules/rnn.py:812\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    811\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 812\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    813\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    814\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    815\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    816\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 210.00 MiB (GPU 1; 11.77 GiB total capacity; 10.27 GiB already allocated; 200.06 MiB free; 10.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "total_train_loss = []\n",
    "total_eval_loss = []\n",
    "# batch = enumerate(list(range(0,10)))\n",
    "for epoch in range(args.epoch):\n",
    "    torch.cuda.empty_cache()\n",
    "    train_loss = 0\n",
    "    # for data in train_dataset:\n",
    "    for i in range(train_loader.__len__()):\n",
    "        data = next(iter(train_loader))\n",
    "        train_loss += train(model=model, data=data, args=args)\n",
    "\n",
    "    train_loss /= train_loader.__len__() # len(train_dataset)\n",
    "    total_train_loss.append(train_loss)\n",
    "\n",
    "    eval_loss = 0\n",
    "    # for data in eval_dataset:\n",
    "    for i in range(test_loader.__len__()):\n",
    "        data = next(iter(test_loader))\n",
    "        eval_loss += eval(model=model, data=data, args=args)\n",
    "    eval_loss /= test_loader.__len__() #len(eval_dataset)\n",
    "    total_eval_loss.append(eval_loss)\n",
    "    \n",
    "    # if (epoch > 25):\n",
    "    #     args.train_lambda = 0.5\n",
    "    print(f'Epoch {epoch}: train loss = {train_loss}; eval loss = {eval_loss}')\n",
    "    if (epoch+1) % 25 == 0:\n",
    "        torch.save(model.state_dict(), f'models/parc_v1_epoch{epoch+1}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(total_train_loss[:100])\n",
    "plt.plot(total_eval_loss[:100])\n",
    "# plt.ylim(0,10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load to evaluate\n",
    "dataset = OneDDatasetLoader(\n",
    "    root_dir='/data1/tam/downloaded_datasets_new',\n",
    "    sub_dir='normalized',\n",
    "    data_names='all',\n",
    "    time_names=[str(i).zfill(3) for i in range(201)]\n",
    ")\n",
    "\n",
    "data = dataset[0]\n",
    "\n",
    "model = PARC(\n",
    "    n_fields=2,\n",
    "    n_timesteps=data.pressure.size(1),\n",
    "    n_meshfields=(data.node_attr.size(1),_test_data.edge_attr.size(1)), # Tuple(n_node_fields, n_mesh_fields)\n",
    "    n_bcfields=1,\n",
    "    n_hiddenfields=args.n_hiddenfields,\n",
    "    n_hiddens=args.n_hiddens\n",
    ").to(args.device)\n",
    "model.load_state_dict(torch.load(\n",
    "    'models/parc_v1_epoch100.pth',\n",
    "    map_location={'cuda:1': 'cuda:0'}\n",
    "))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.n_timesteps = data.pressure.size(1)\n",
    "timestep = args.total_time / model.n_timesteps\n",
    "\n",
    "## Connectivity/edge_index: Tensor(2, n_edges)\n",
    "edge_index = torch.cat([\n",
    "    data.edge_index, \n",
    "    torch.flip(data.edge_index, dims=[0]\n",
    ")], dim=1).to(args.device)\n",
    "\n",
    "## Mesh features: Tuple(NodeTensor, EdgeTensor)\n",
    "mesh_features = (\n",
    "    data.node_attr.to(args.device).float(),            \n",
    "    torch.cat([data.edge_attr.to(args.device).float()]*2,dim=0)\n",
    ")\n",
    "\n",
    "## Fields tensor(pressure, flowrate): Tensor(n_nodes, n_times, n_fields)\n",
    "F_initial = torch.cat([\n",
    "    data.pressure[:,0].unsqueeze(1), \n",
    "    data.flowrate[:,0].unsqueeze(1)\n",
    "], dim=-1).to(args.device).float() # concat pressure and flowrate\n",
    "\n",
    "## Boundary value tensor: Tensor(n_nodes, n_times)\n",
    "F_bc = torch.zeros((data.number_of_nodes, model.n_timesteps))\n",
    "F_bc[0,:] = data.flowrate[0,:]\n",
    "F_bc = F_bc.to(args.device).float()\n",
    "\n",
    "## Predict output\n",
    "with torch.no_grad():\n",
    "    Fs, F_dots = model(\n",
    "        F_initial=F_initial, \n",
    "        mesh_features=mesh_features, \n",
    "        edge_index=edge_index, \n",
    "        F_boundary=F_bc, \n",
    "        timestep=timestep\n",
    "    )\n",
    "\n",
    "    ## Ground truth Fields tensor(pressure, flowrate): Tensor(n_nodes, 1:n_times, n_fields)\n",
    "    Fs_hat = torch.cat([\n",
    "        data.pressure.unsqueeze(-1), \n",
    "        data.flowrate.unsqueeze(-1)\n",
    "    ], dim=-1).to(args.device).float() # concat pressure and flowrate\n",
    "    Fs_hat = Fs_hat[:,1:,:]\n",
    "    \n",
    "    ## Ground truth Fields time derivative tensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "for i_node in [1000, 2000, 10000]:\n",
    "    i_field = 1\n",
    "    y_pred = Fs.cpu().numpy()[i_node,:,i_field]\n",
    "    y_true = Fs_hat.cpu().numpy()[i_node,:,i_field]\n",
    "    x = [i * 4.0 /200 for i in range(y_pred.shape[0])]\n",
    "    # print(data.node_attr.numpy()[i_node, 6])\n",
    "    # plt.ylim(-1,1)\n",
    "    plt.plot(x, y_pred, c='red', label='GNN Crank-Nicolson')\n",
    "    plt.plot(x, y_true, c='blue', linestyle='dashdot', label='ground_truth')\n",
    "    # plt.ylim([-1,1])\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Flowrate', fontsize=20)\n",
    "    plt.xlabel('Time', fontsize=20)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
