{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../Codes/')\n",
    "import torch\n",
    "from dataset import OneDDatasetBuilder, OneDDatasetLoader\n",
    "from plot import *\n",
    "from preprocessing import dataset_split_to_loader, cal_deriv_F\n",
    "from networks_v3 import PARC_Graph\n",
    "\n",
    "import torch_geometric\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n",
    "# %env CUDA_VISIBLE_DEVICE=2\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define argument\n",
    "class objectview(object):\n",
    "    def __init__(self, d) -> None:\n",
    "        self.__dict__ = d\n",
    "    def setattr(self, attr_name, attr_value):\n",
    "        self.__dict__[attr_name] = attr_value\n",
    "\n",
    "args = objectview(d={\n",
    "    # data params\n",
    "    'total_time': 4.0,\n",
    "    'n_time': 201,\n",
    "    'batch_size': None,\n",
    "    'batch_n_time': None,\n",
    "    'batch_step': 2,\n",
    "    'batch_recursive': True,\n",
    "    # model params\n",
    "    'n_field': 2,\n",
    "    'n_meshfield': (3, 15),\n",
    "    'n_boundaryfield': 1,\n",
    "    'hidden_size': 256,\n",
    "    'unet_depth':5,\n",
    "    'forward_sequence': False,\n",
    "    'max_gen': 10,\n",
    "    # training params\n",
    "    'device': torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    'lr': 2e-7,\n",
    "    'weight_decay': 5e-3,\n",
    "    'epoch': 2000,\n",
    "    'criterion': torch.nn.MSELoss(),\n",
    "    'n_datas_per_batch': 1,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build dataset\n",
    "# dataset = OneDDatasetBuilder(\n",
    "#     raw_dir='/data1/tam/datasets_231228',\n",
    "#     root_dir='/data1/tam/downloaded_datasets_WT_v1',\n",
    "#     data_names='all',\n",
    "#     time_names=[str(i).zfill(3) for i in range(201)]\n",
    "# )\n",
    "\n",
    "# Load dataset\n",
    "dataset = OneDDatasetLoader(\n",
    "    root_dir='M:/Tam/fromHung_data231228_source/download_dataset_WT_v1',\n",
    "    sub_dir='processed',\n",
    "    data_names='all',\n",
    "    time_names=[str(i).zfill(3) for i in range(201)]\n",
    ")\n",
    "\n",
    "# dataset = dataset.cut_branch(max_gen=args.max_gen)\n",
    "\n",
    "# dataset = dataset.normalizing(\n",
    "#     sub_dir='normalized',\n",
    "#     scalers = {\n",
    "#         'node_attr' : ['minmax_scaler', 0],\n",
    "#         'edge_attr' : ['minmax_scaler', 0],\n",
    "#         'pressure' : ['minmax_scaler', None],\n",
    "#         'flowrate' : ['minmax_scaler', None],\n",
    "#         'pressure_dot' : ['minmax_scaler', None],\n",
    "#         'flowrate_dot' : ['minmax_scaler', None]\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# dataset = dataset.batching(\n",
    "#     batch_size = args.batch_size,\n",
    "#     batch_n_times = args.batch_n_time, \n",
    "#     recursive = args.batch_recursive, \n",
    "#     sub_dir='/normed_batched', \n",
    "#     step=args.batch_step\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([59692, 201])\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "from loss import OneDAirwayLoss\n",
    "criterion = OneDAirwayLoss()\n",
    "data = dataset[1]\n",
    "loss = criterion(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss[:,0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = OneDDatasetLoader(\n",
    "    root_dir='/data1/tam/downloaded_datasets_WT_v1',\n",
    "    sub_dir='normed_batched',\n",
    "    data_names='all',\n",
    "    time_names=[str(i).zfill(3) for i in range(201)]\n",
    ")\n",
    "\n",
    "train_loader, test_loader = dataset_split_to_loader(\n",
    "    dataset = dataset,\n",
    "    subset_ids = {\n",
    "        'train': list(range(0, 30)),\n",
    "        'test': list(range(30, 35))\n",
    "    },\n",
    "    n_datas_per_batch = args.n_datas_per_batch\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare model\n",
    "model = PARC_Graph(\n",
    "    n_field=args.n_field,\n",
    "    n_meshfield=args.n_meshfield,\n",
    "    n_boundaryfield=args.n_boundaryfield,\n",
    "    hidden_size=args.hidden_size,\n",
    "    unet_depth=args.unet_depth,\n",
    "    activation=torch.nn.functional.relu\n",
    ")\n",
    "# model = torch_geometric.nn.DataParallel(model)\n",
    "model = model.to(args.device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "# args.setattr(attr_name='optimizer', attr_value=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, args, forward_sequence=False):\n",
    "    ## Field tensor: Tensor(n_node, n_time, n_field)\n",
    "    F_true = torch.cat([\n",
    "        data.pressure.unsqueeze(2),\n",
    "        data.flowrate.unsqueeze(2)\n",
    "    ], dim=2).float().to(args.device)\n",
    "    if not forward_sequence:\n",
    "        # forward only initial state\n",
    "        F = F_true[:,0,:]\n",
    "    else:\n",
    "        # forward all timestep\n",
    "        F = F_true\n",
    "    F_true = F_true[:,1:,:] # timestep 1 to N\n",
    "\n",
    "    ## Field gradient tensor\n",
    "    F_dot_true = torch.cat([\n",
    "        data.pressure_dot.unsqueeze(2),\n",
    "        data.flowrate_dot.unsqueeze(2)\n",
    "    ], dim=2).float().to(args.device)\n",
    "    F_dot_true = F_dot_true[:,:-1,:] # timestep 0 to N-1\n",
    "    \n",
    "    ## Connectivity/edge_index: Tensor(2, n_edge)\n",
    "    edge_index = data.edge_index.to(args.device)\n",
    "    # edge_index = torch.cat([\n",
    "    #     data.edge_index, torch.flip(data.edge_index, dims=[0]\n",
    "    # )], dim=1).to(args.device)\n",
    "\n",
    "    ## Mesh features: Tuple(Tensor(n_node, n_node_attr), Tensor(n_edge, n_edge_attr))\n",
    "    node_attr = data.node_attr.float().to(args.device)\n",
    "    edge_attr = data.edge_attr.float().to(args.device)\n",
    "    # edge_attr = torch.cat([data.edge_attr, data.edge_attr], dim=0).float().to(args.device)\n",
    "    meshfield = (node_attr, edge_attr)\n",
    "\n",
    "    # Boundary field\n",
    "    boundaryfield = torch.cat([data.flowrate[0,:].unsqueeze(0)]*data.flowrate.size(0), dim=0)\n",
    "    boundaryfield = boundaryfield.float().to(args.device)\n",
    "    # boundaryfield = None\n",
    "\n",
    "\n",
    "    ## Predict output sequence\n",
    "    F_pred, F_dot_pred = model(\n",
    "        F_input = F,\n",
    "        edge_index=edge_index,\n",
    "        meshfield=meshfield,\n",
    "        boundaryfield=boundaryfield,\n",
    "        forward_sequence=forward_sequence,\n",
    "        n_time=data.flowrate.size(1)\n",
    "    )\n",
    "    \n",
    "    ## loss calculation\n",
    "    loss = args.criterion(F_pred, F_true) + args.criterion(F_dot_pred, F_dot_true)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, data, args, forward_sequence=False):\n",
    "    ## Field tensor: Tensor(n_node, n_time, n_field)\n",
    "    F_true = torch.cat([\n",
    "            data.pressure.unsqueeze(2),\n",
    "            data.flowrate.unsqueeze(2)\n",
    "        ], dim=2).float().to(args.device)\n",
    "    if not forward_sequence:\n",
    "        # forward only initial state\n",
    "        F = F_true[:,0,:]\n",
    "    else:\n",
    "        # forward all timestep\n",
    "        F = F_true\n",
    "    F_true = F_true[:,1:,:]\n",
    "\n",
    "    ## Field gradient tensor\n",
    "    F_dot_true = torch.cat([\n",
    "        data.pressure_dot.unsqueeze(2),\n",
    "        data.flowrate_dot.unsqueeze(2)\n",
    "    ], dim=2).float().to(args.device)\n",
    "    F_dot_true = F_dot_true[:,:-1,:] # timestep 0 to N-1\n",
    "    \n",
    "    ## Connectivity/edge_index: Tensor(2, n_edge)\n",
    "    edge_index = data.edge_index.to(args.device)\n",
    "    # edge_index = torch.cat([\n",
    "    #     data.edge_index, torch.flip(data.edge_index, dims=[0]\n",
    "    # )], dim=1).to(args.device)\n",
    "\n",
    "    ## Mesh features: Tuple(Tensor(n_node, n_node_attr), Tensor(n_edge, n_edge_attr))\n",
    "    node_attr = data.node_attr.float().to(args.device)\n",
    "    edge_attr = data.edge_attr.float().to(args.device)\n",
    "    # edge_attr = torch.cat([data.edge_attr, data.edge_attr], dim=0).float().to(args.device)\n",
    "    meshfield = (node_attr, edge_attr)\n",
    "\n",
    "    ## Boundary field\n",
    "    boundaryfield = torch.cat([data.flowrate[0,:].unsqueeze(0)]*data.flowrate.size(0), dim=0)\n",
    "    boundaryfield = boundaryfield.float().to(args.device)\n",
    "    # boundaryfield = None\n",
    "\n",
    "    ## Predict output sequence\n",
    "    with torch.no_grad():\n",
    "        F_pred, F_dot_pred = model(\n",
    "            F_input = F,\n",
    "            edge_index=edge_index,\n",
    "            meshfield=meshfield,\n",
    "            boundaryfield=boundaryfield,\n",
    "            forward_sequence=forward_sequence,\n",
    "            n_time=data.flowrate.size(1)\n",
    "        )\n",
    "        \n",
    "        ## loss calculation\n",
    "        loss = args.criterion(F_pred, F_true) + args.criterion(F_dot_pred, F_dot_true)\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "total_train_loss = []\n",
    "total_eval_loss = []\n",
    "# batch = enumerate(list(range(0,10)))\n",
    "for epoch in range(args.epoch):\n",
    "    torch.cuda.empty_cache()\n",
    "    train_loss = 0\n",
    "    for i in range(train_loader.__len__()):\n",
    "        data = next(iter(train_loader))\n",
    "        # print(data)\n",
    "        train_loss += train(model=model, data=data, args=args, forward_sequence=True)\n",
    "\n",
    "    train_loss /= train_loader.__len__() # len(train_dataset)\n",
    "    total_train_loss.append(train_loss)\n",
    "\n",
    "    eval_loss = 0\n",
    "    for i in range(test_loader.__len__()):\n",
    "        data = next(iter(test_loader))\n",
    "        eval_loss += eval(model=model, data=data, args=args)\n",
    "    eval_loss /= test_loader.__len__() #len(eval_dataset)\n",
    "    total_eval_loss.append(eval_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch}: train loss = {train_loss}; eval loss = {eval_loss}')\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        torch.save(model.state_dict(), f'models/parc_3_epoch{epoch+1}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_sequence_validate = False\n",
    "# Load to evaluate\n",
    "dataset = OneDDatasetLoader(\n",
    "    root_dir='/data1/tam/downloaded_datasets_WT_v1',\n",
    "    sub_dir='normed_batched',\n",
    "    # sub_dir='normed_and_batched',\n",
    "    data_names='all',\n",
    "    time_names=[str(i).zfill(3) for i in range(201)]\n",
    ")\n",
    "\n",
    "data = dataset[38]\n",
    "# args.device = torch.device('cpu')\n",
    "# prepare model\n",
    "model = PARC_Graph(\n",
    "    n_field=args.n_field,\n",
    "    n_meshfield=args.n_meshfield,\n",
    "    n_boundaryfield=args.n_boundaryfield,\n",
    "    hidden_size=args.hidden_size,\n",
    "    unet_depth=args.unet_depth,\n",
    "    activation=torch.nn.functional.relu\n",
    ")\n",
    "model.load_state_dict(torch.load(\n",
    "    'models/parc_3_epoch640.pth',\n",
    "    map_location='cuda:1'\n",
    "))\n",
    "model=model.to(args.device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "F_true = torch.cat([\n",
    "        data.pressure.unsqueeze(2),\n",
    "        data.flowrate.unsqueeze(2)\n",
    "    ], dim=2).float().to(args.device)\n",
    "if not args.forward_sequence:\n",
    "    # forward only initial state\n",
    "    F = F_true[:,0,:]\n",
    "else:\n",
    "    # forward all timestep\n",
    "    F = F_true\n",
    "F_true = F_true[:,1:,:]\n",
    "\n",
    "## Field gradient tensor\n",
    "F_dot_true = torch.cat([\n",
    "    data.pressure_dot.unsqueeze(2),\n",
    "    data.flowrate_dot.unsqueeze(2)\n",
    "], dim=2).float().to(args.device)\n",
    "F_dot_true = F_dot_true[:,:-1,:] # timestep 0 to N-1\n",
    "\n",
    "## Connectivity/edge_index: Tensor(2, n_edge)\n",
    "edge_index = data.edge_index.to(args.device)\n",
    "# edge_index = torch.cat([\n",
    "#     data.edge_index, torch.flip(data.edge_index, dims=[0]\n",
    "# )], dim=1).to(args.device)\n",
    "\n",
    "## Mesh features: Tuple(Tensor(n_node, n_node_attr), Tensor(n_edge, n_edge_attr))\n",
    "node_attr = data.node_attr.float().to(args.device)\n",
    "edge_attr = data.edge_attr.float().to(args.device)\n",
    "# edge_attr = torch.cat([data.edge_attr, data.edge_attr], dim=0).float().to(args.device)\n",
    "meshfield = (node_attr, edge_attr)\n",
    "\n",
    "## Boundary field\n",
    "boundaryfield = torch.cat([data.flowrate[0,:].unsqueeze(0)]*data.flowrate.size(0), dim=0)\n",
    "boundaryfield = boundaryfield.float().to(args.device)\n",
    "# boundaryfield = None\n",
    "\n",
    "## Predict output sequence\n",
    "with torch.no_grad():\n",
    "    F_pred, F_dot_pred = model(\n",
    "        F_input = F,\n",
    "        edge_index=edge_index,\n",
    "        meshfield=meshfield,\n",
    "        boundaryfield=boundaryfield,\n",
    "        forward_sequence=args.forward_sequence,\n",
    "        n_time=data.flowrate.size(1)\n",
    "    )\n",
    "\n",
    "node_list = [5, 10, 50, 100, 150, 200]\n",
    "## Draw pressure\n",
    "import matplotlib.pyplot as plt\n",
    "for i_node in node_list:\n",
    "    i_field = 0\n",
    "    y_pred = F_pred.cpu().numpy()[i_node,:,i_field]\n",
    "    y_true = F_true.cpu().numpy()[i_node,:,i_field]\n",
    "    # print(y_true, y_pred)\n",
    "    x = [i * 4.0 /200 for i in range(y_pred.shape[0])]\n",
    "    # print(data.node_attr.numpy()[i_node, 6])\n",
    "    # plt.ylim(-1,1)\n",
    "    plt.plot(x, y_pred, c='red', label='GNN Euler')\n",
    "    plt.plot(x, y_true, c='blue', linestyle='dashdot', label='ground_truth')\n",
    "    # plt.ylim([-1,1])\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Pressure', fontsize=20)\n",
    "    plt.xlabel('Time', fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "## Draw flowrate\n",
    "for i_node in node_list:\n",
    "    i_field = 1\n",
    "    y_pred = F_pred.cpu().numpy()[i_node,:,i_field]\n",
    "    y_true = F_true.cpu().numpy()[i_node,:,i_field]\n",
    "    x = [i * 4.0 /200 for i in range(y_pred.shape[0])]\n",
    "    # print(data.node_attr.numpy()[i_node, 6])\n",
    "    # plt.ylim(-1,1)\n",
    "    plt.plot(x, y_pred, c='red', label='GNN Euler')\n",
    "    plt.plot(x, y_true, c='blue', linestyle='dashdot', label='ground_truth')\n",
    "    # plt.ylim([-1,1])\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Flowrate', fontsize=20)\n",
    "    plt.xlabel('Time', fontsize=20)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
