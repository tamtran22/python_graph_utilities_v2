{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/data1/tam/python_graph_utilities_v2/Codes/')\n",
    "import torch\n",
    "from dataset import OneDDatasetBuilder, OneDDatasetLoader\n",
    "from plot import *\n",
    "from preprocessing import dataset_split_to_loader\n",
    "from networks_v2 import MeshGraphNet, RecurrentFormulationNetwork\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define argument\n",
    "class objectview(object):\n",
    "    def __init__(self, d) -> None:\n",
    "        self.__dict__ = d\n",
    "    def setattr(self, attr_name, attr_value):\n",
    "        self.__dict__[attr_name] = attr_value\n",
    "\n",
    "args = objectview(d={\n",
    "    # data params\n",
    "    'total_time': 4.8,\n",
    "    'n_time': 201,\n",
    "    'batch_size': 1000,\n",
    "    'batch_n_time': 25,\n",
    "    'batch_step': 1,\n",
    "    'batch_recursive': True,\n",
    "    # model params\n",
    "    'n_field': 2,\n",
    "    'n_meshfield': (3, 8),\n",
    "    'n_boundaryfield': 0,\n",
    "    'latent_size': 10,\n",
    "    'n_latent': 10,\n",
    "    'hidden_size': 128,\n",
    "    'n_hidden': 5,\n",
    "    'forward_sequence': False,\n",
    "    # training params\n",
    "    'device': torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    'lr': 1e-7,\n",
    "    'weight_decay': 5e-4,\n",
    "    'epoch': 200,\n",
    "    'criterion': torch.nn.MSELoss(),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build dataset\n",
    "# dataset = OneDDatasetBuilder(\n",
    "#     raw_dir='/data1/tam/datasets',\n",
    "#     root_dir='/data1/tam/downloaded_datasets_new1',\n",
    "#     data_names='all',\n",
    "#     time_names=[str(i).zfill(3) for i in range(201)]\n",
    "# )\n",
    "\n",
    "# Load dataset\n",
    "dataset = OneDDatasetLoader(\n",
    "    root_dir='/data1/tam/downloaded_datasets_new1',\n",
    "    sub_dir='normalized',\n",
    "    data_names='all',\n",
    "    time_names=[str(i).zfill(3) for i in range(201)]\n",
    ")\n",
    "\n",
    "# dataset = dataset.normalizing(\n",
    "#     sub_dir='normalized',\n",
    "#     scalers = {\n",
    "#         'node_attr' : ['minmax_scaler', 0],\n",
    "#         'edge_attr' : ['quantile_transformer', 0],\n",
    "#         'pressure' : ['quantile_transformer', None],\n",
    "#         'flowrate' : ['quantile_transformer', None]\n",
    "#     }\n",
    "# )\n",
    "\n",
    "dataset = dataset.batching(\n",
    "    batch_size = args.batch_size,\n",
    "    batch_n_times = args.batch_n_time, \n",
    "    recursive = args.batch_recursive, \n",
    "    sub_dir='/two_timesteps', \n",
    "    step=args.batch_step\n",
    ")\n",
    "\n",
    "# # Prepare data\n",
    "# train_loader, test_loader = dataset_split_to_loader(\n",
    "#     dataset = dataset,\n",
    "#     subset_ids = {\n",
    "#         'train': list(range(0, 15)),\n",
    "#         'test': list(range(20, 35))\n",
    "#     },\n",
    "#     n_datas_per_batch = 1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare model\n",
    "model = RecurrentFormulationNetwork(\n",
    "    n_field=args.n_field,\n",
    "    n_meshfield=args.n_meshfield,\n",
    "    n_boundaryfield=args.n_boundaryfield,\n",
    "    n_hidden=args.n_hidden,\n",
    "    hidden_size=args.hidden_size,\n",
    "    n_latent=args.n_latent,\n",
    "    latent_size=args.latent_size,\n",
    "    integration=None\n",
    ").to(args.device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "args.setattr(attr_name='optimizer', attr_value=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, args, forward_sequence=False):\n",
    "    ## Field tensor: Tensor(n_node, n_time, n_field)\n",
    "    F_true = torch.cat([\n",
    "            data.pressure.unsqueeze(2),\n",
    "            data.flowrate.unsqueeze(2)\n",
    "        ], dim=2).float().to(args.device)\n",
    "    if not forward_sequence:\n",
    "        # forward only initial state\n",
    "        # F = torch.cat([\n",
    "        #     data.pressure[:,0].unsqueeze(1),\n",
    "        #     data.flowrate[:,0].unsqueeze(1)\n",
    "        # ], dim=1).float().to(args.device)\n",
    "        F = F_true[:,0,:]\n",
    "    else:\n",
    "        # forward all timestep\n",
    "        F = F_true\n",
    "    F_true = F_true[:,1:,:]\n",
    "    \n",
    "    ## Connectivity/edge_index: Tensor(2, n_edge)\n",
    "    edge_index = data.edge_index.to(args.device)\n",
    "\n",
    "    ## Mesh features: Tuple(Tensor(n_node, n_node_attr), Tensor(n_edge, n_edge_attr))\n",
    "    node_attr = data.node_attr.float().to(args.device)\n",
    "    edge_attr = data.edge_attr.float().to(args.device)\n",
    "    meshfield = (node_attr, edge_attr)\n",
    "\n",
    "    ## Boundary field\n",
    "    boundaryfield = None\n",
    "\n",
    "    ## Time tensor\n",
    "    # time = torch.zeros(data.pressure.size())\n",
    "    # timestep = args.total_time / (args.n_time - 1)\n",
    "    # n_time = time.size(1)\n",
    "    # for i in range(n_time):\n",
    "    #     time[:,i] = i * timestep\n",
    "    # time = time.float().to(args.device)\n",
    "    time = data.time.float().to(args.device)\n",
    "\n",
    "    ## Predict output sequence\n",
    "    F_pred = model(\n",
    "        F = F,\n",
    "        edge_index=edge_index,\n",
    "        meshfield=meshfield,\n",
    "        boundaryfield=boundaryfield,\n",
    "        time=time,\n",
    "        forward_sequence=forward_sequence\n",
    "    )\n",
    "    \n",
    "    ## loss calculation\n",
    "    # print(F_true.size(), F_pred.size())\n",
    "    loss = args.criterion(F_true, F_pred)\n",
    "    loss.backward()\n",
    "    args.optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "# train(model, dataset[0], args, forward_sequence=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "total_train_loss = []\n",
    "total_eval_loss = []\n",
    "# batch = enumerate(list(range(0,10)))\n",
    "for epoch in range(args.epoch):\n",
    "    torch.cuda.empty_cache()\n",
    "    train_loss = 0\n",
    "    for i in range(train_loader.__len__()):\n",
    "        data = next(iter(train_loader))\n",
    "        train_loss += train(model=model, data=data, args=args, forward_sequence=True)\n",
    "\n",
    "    train_loss /= train_loader.__len__() # len(train_dataset)\n",
    "    total_train_loss.append(train_loss)\n",
    "\n",
    "    eval_loss = 0\n",
    "    # # for data in eval_dataset:\n",
    "    # for i in range(test_loader.__len__()):\n",
    "    #     data = next(iter(test_loader))\n",
    "    #     eval_loss += eval(model=model, data=data, args=args)\n",
    "    # eval_loss /= test_loader.__len__() #len(eval_dataset)\n",
    "    # total_eval_loss.append(eval_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch}: train loss = {train_loss}; eval loss = {eval_loss}')\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        torch.save(model.state_dict(), f'models/parc_test_epoch{epoch+1}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on CUDA device 0 but torch.cuda.device_count() is 0. Please use torch.load with map_location to map your storages to an existing device.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/data1/tam/python_graph_utilities_v2/Run/run_train_mgn_1.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B155.230.94.82/data1/tam/python_graph_utilities_v2/Run/run_train_mgn_1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# args.device = torch.device('cpu')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B155.230.94.82/data1/tam/python_graph_utilities_v2/Run/run_train_mgn_1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# prepare model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B155.230.94.82/data1/tam/python_graph_utilities_v2/Run/run_train_mgn_1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m model \u001b[39m=\u001b[39m RecurrentFormulationNetwork(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B155.230.94.82/data1/tam/python_graph_utilities_v2/Run/run_train_mgn_1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     n_field\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mn_field,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B155.230.94.82/data1/tam/python_graph_utilities_v2/Run/run_train_mgn_1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     n_meshfield\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mn_meshfield,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B155.230.94.82/data1/tam/python_graph_utilities_v2/Run/run_train_mgn_1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     integration\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B155.230.94.82/data1/tam/python_graph_utilities_v2/Run/run_train_mgn_1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B155.230.94.82/data1/tam/python_graph_utilities_v2/Run/run_train_mgn_1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B155.230.94.82/data1/tam/python_graph_utilities_v2/Run/run_train_mgn_1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mmodels/parc_test_epoch100.pth\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B155.230.94.82/data1/tam/python_graph_utilities_v2/Run/run_train_mgn_1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     map_location\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcuda:0\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B155.230.94.82/data1/tam/python_graph_utilities_v2/Run/run_train_mgn_1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m ))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B155.230.94.82/data1/tam/python_graph_utilities_v2/Run/run_train_mgn_1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m F_true \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B155.230.94.82/data1/tam/python_graph_utilities_v2/Run/run_train_mgn_1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m         data\u001b[39m.\u001b[39mpressure\u001b[39m.\u001b[39munsqueeze(\u001b[39m2\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B155.230.94.82/data1/tam/python_graph_utilities_v2/Run/run_train_mgn_1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m         data\u001b[39m.\u001b[39mflowrate\u001b[39m.\u001b[39munsqueeze(\u001b[39m2\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B155.230.94.82/data1/tam/python_graph_utilities_v2/Run/run_train_mgn_1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     ], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(args\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B155.230.94.82/data1/tam/python_graph_utilities_v2/Run/run_train_mgn_1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m args\u001b[39m.\u001b[39mforward_sequence:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B155.230.94.82/data1/tam/python_graph_utilities_v2/Run/run_train_mgn_1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39m# forward only initial state\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B155.230.94.82/data1/tam/python_graph_utilities_v2/Run/run_train_mgn_1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39m# F = torch.cat([\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B155.230.94.82/data1/tam/python_graph_utilities_v2/Run/run_train_mgn_1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39m#     data.pressure[:,0].unsqueeze(1),\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B155.230.94.82/data1/tam/python_graph_utilities_v2/Run/run_train_mgn_1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39m#     data.flowrate[:,0].unsqueeze(1)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B155.230.94.82/data1/tam/python_graph_utilities_v2/Run/run_train_mgn_1.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m     \u001b[39m# ], dim=1).float().to(args.device)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/geometric/lib/python3.11/site-packages/torch/serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    808\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 809\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    810\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[1;32m    811\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/geometric/lib/python3.11/site-packages/torch/serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1170\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1171\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1172\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1174\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1176\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/geometric/lib/python3.11/site-packages/torch/serialization.py:1142\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1141\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1142\u001b[0m     typed_storage \u001b[39m=\u001b[39m load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   1144\u001b[0m \u001b[39mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/anaconda3/envs/geometric/lib/python3.11/site-packages/torch/serialization.py:1116\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1112\u001b[0m storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39mUntypedStorage)\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_untyped_storage\n\u001b[1;32m   1113\u001b[0m \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m typed_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1116\u001b[0m     wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1117\u001b[0m     dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m   1118\u001b[0m     _internal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1120\u001b[0m \u001b[39mif\u001b[39;00m typed_storage\u001b[39m.\u001b[39m_data_ptr() \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1121\u001b[0m     loaded_storages[key] \u001b[39m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m~/anaconda3/envs/geometric/lib/python3.11/site-packages/torch/serialization.py:1083\u001b[0m, in \u001b[0;36m_get_restore_location.<locals>.restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m   1082\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrestore_location\u001b[39m(storage, location):\n\u001b[0;32m-> 1083\u001b[0m     \u001b[39mreturn\u001b[39;00m default_restore_location(storage, map_location)\n",
      "File \u001b[0;32m~/anaconda3/envs/geometric/lib/python3.11/site-packages/torch/serialization.py:217\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 217\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[1;32m    218\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/geometric/lib/python3.11/site-packages/torch/serialization.py:182\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    181\u001b[0m     \u001b[39mif\u001b[39;00m location\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 182\u001b[0m         device \u001b[39m=\u001b[39m validate_cuda_device(location)\n\u001b[1;32m    183\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m_torch_load_uninitialized\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    184\u001b[0m             \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(device):\n",
      "File \u001b[0;32m~/anaconda3/envs/geometric/lib/python3.11/site-packages/torch/serialization.py:173\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    171\u001b[0m device_count \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count()\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m device_count:\n\u001b[0;32m--> 173\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAttempting to deserialize object on CUDA device \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    174\u001b[0m                        \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mdevice\u001b[39m}\u001b[39;00m\u001b[39m but torch.cuda.device_count() is \u001b[39m\u001b[39m{\u001b[39;00mdevice_count\u001b[39m}\u001b[39;00m\u001b[39m. Please use \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    175\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mtorch.load with map_location to map your storages \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    176\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mto an existing device.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    177\u001b[0m \u001b[39mreturn\u001b[39;00m device\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on CUDA device 0 but torch.cuda.device_count() is 0. Please use torch.load with map_location to map your storages to an existing device."
     ]
    }
   ],
   "source": [
    "# Load to evaluate\n",
    "dataset = OneDDatasetLoader(\n",
    "    root_dir='/data1/tam/downloaded_datasets_new1',\n",
    "    sub_dir='normalized',\n",
    "    # sub_dir='normed_and_batched',\n",
    "    data_names='all',\n",
    "    time_names=[str(i).zfill(3) for i in range(201)]\n",
    ")\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "data = dataset[0]\n",
    "# args.device = torch.device('cpu')\n",
    "# prepare model\n",
    "model = RecurrentFormulationNetwork(\n",
    "    n_field=args.n_field,\n",
    "    n_meshfield=args.n_meshfield,\n",
    "    n_boundaryfield=args.n_boundaryfield,\n",
    "    n_hidden=args.n_hidden,\n",
    "    hidden_size=args.hidden_size,\n",
    "    n_latent=args.n_latent,\n",
    "    latent_size=args.latent_size,\n",
    "    integration=None\n",
    ")\n",
    "model.load_state_dict(torch.load(\n",
    "    'models/parc_test_epoch100.pth',\n",
    "    map_location='cuda:0'\n",
    "))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "F_true = torch.cat([\n",
    "        data.pressure.unsqueeze(2),\n",
    "        data.flowrate.unsqueeze(2)\n",
    "    ], dim=2).float().to(args.device)\n",
    "if not args.forward_sequence:\n",
    "    # forward only initial state\n",
    "    # F = torch.cat([\n",
    "    #     data.pressure[:,0].unsqueeze(1),\n",
    "    #     data.flowrate[:,0].unsqueeze(1)\n",
    "    # ], dim=1).float().to(args.device)\n",
    "    F = F_true[:,0,:]\n",
    "else:\n",
    "    # forward all timestep\n",
    "    F = F_true\n",
    "F_true = F_true[:,1:,:]\n",
    "\n",
    "## Connectivity/edge_index: Tensor(2, n_edge)\n",
    "edge_index = data.edge_index.to(args.device)\n",
    "\n",
    "## Mesh features: Tuple(Tensor(n_node, n_node_attr), Tensor(n_edge, n_edge_attr))\n",
    "node_attr = data.node_attr.float().to(args.device)\n",
    "edge_attr = data.edge_attr.float().to(args.device)\n",
    "meshfield = (node_attr, edge_attr)\n",
    "\n",
    "## Boundary field\n",
    "boundaryfield = None\n",
    "\n",
    "## Time tensor\n",
    "time = data.time.float().to(args.device)\n",
    "\n",
    "## Predict output sequence\n",
    "with torch.no_grad():\n",
    "    F_pred = model(\n",
    "        F = F,\n",
    "        edge_index=edge_index,\n",
    "        meshfield=meshfield,\n",
    "        boundaryfield=boundaryfield,\n",
    "        time=time,\n",
    "        forward_sequence=args.forward_sequence\n",
    "    )\n",
    "\n",
    "\n",
    "node_list = [10, 300, 5000]\n",
    "## Draw pressure\n",
    "import matplot.pyplot as plt\n",
    "for i_node in node_list:\n",
    "    i_field = 0\n",
    "    y_pred = F_pred.cpu().numpy()[i_node,:,i_field]\n",
    "    y_true = F_true.cpu().numpy()[i_node,:,i_field]\n",
    "    x = [i * 4.0 /200 for i in range(y_pred.shape[0])]\n",
    "    # print(data.node_attr.numpy()[i_node, 6])\n",
    "    # plt.ylim(-1,1)\n",
    "    plt.plot(x, y_pred, c='red', label='GNN Euler')\n",
    "    plt.plot(x, y_true, c='blue', linestyle='dashdot', label='ground_truth')\n",
    "    # plt.ylim([-1,1])\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Pressure', fontsize=20)\n",
    "    plt.xlabel('Time', fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "## Draw flowrate\n",
    "for i_node in node_list:\n",
    "    i_field = 1\n",
    "    y_pred = F_pred.cpu().numpy()[i_node,:,i_field]\n",
    "    y_true = F_true.cpu().numpy()[i_node,:,i_field]\n",
    "    x = [i * 4.0 /200 for i in range(y_pred.shape[0])]\n",
    "    # print(data.node_attr.numpy()[i_node, 6])\n",
    "    # plt.ylim(-1,1)\n",
    "    plt.plot(x, y_pred, c='red', label='GNN Euler')\n",
    "    plt.plot(x, y_true, c='blue', linestyle='dashdot', label='ground_truth')\n",
    "    # plt.ylim([-1,1])\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Flowrate', fontsize=20)\n",
    "    plt.xlabel('Time', fontsize=20)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geometric",
   "language": "python",
   "name": "geometric"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
