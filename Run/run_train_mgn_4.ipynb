{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/data1/tam/python_graph_utilities_v2/Codes/')\n",
    "import torch\n",
    "from dataset import OneDDatasetBuilder, OneDDatasetLoader\n",
    "from plot import *\n",
    "from preprocessing import dataset_split_to_loader\n",
    "from networks_v4 import RecurrentFormulationNetwork\n",
    "\n",
    "import torch_geometric\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n",
    "# %env CUDA_VISIBLE_DEVICE=2\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define argument\n",
    "class objectview(object):\n",
    "    def __init__(self, d) -> None:\n",
    "        self.__dict__ = d\n",
    "    def setattr(self, attr_name, attr_value):\n",
    "        self.__dict__[attr_name] = attr_value\n",
    "\n",
    "args = objectview(d={\n",
    "    # data params\n",
    "    'total_time': 4.0,\n",
    "    'n_time': 201,\n",
    "    'batch_size': 2000,\n",
    "    'batch_n_time': None,\n",
    "    'batch_step': 1,\n",
    "    'batch_recursive': True,\n",
    "    # model params\n",
    "    'n_field': 2,\n",
    "    'n_meshfield': (19, 0),\n",
    "    'n_boundaryfield': 1,\n",
    "    'n_globalfield': 0,\n",
    "    'latent_size': 128,\n",
    "    'n_latent': 10,\n",
    "    'hidden_size': 256,\n",
    "    'n_hidden': 5,\n",
    "    'forward_sequence': False,\n",
    "    # training params\n",
    "    'device': torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    'lr': 5e-7,\n",
    "    'weight_decay': 5e-4,\n",
    "    'epoch': 100,\n",
    "    'criterion': torch.nn.MSELoss(),\n",
    "    'n_datas_per_batch': 1,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build dataset\n",
    "# dataset = OneDDatasetBuilder(\n",
    "#     raw_dir='/data1/tam/datasets',\n",
    "#     root_dir='/data1/tam/downloaded_datasets_new1',\n",
    "#     data_names='all',\n",
    "#     time_names=[str(i).zfill(3) for i in range(201)]\n",
    "# )\n",
    "\n",
    "# Load dataset\n",
    "dataset = OneDDatasetLoader(\n",
    "    root_dir='/data1/tam/downloaded_datasets_WT_v2',\n",
    "    sub_dir='processed',\n",
    "    data_names='all',\n",
    "    time_names=[str(i).zfill(3) for i in range(201)]\n",
    ")\n",
    "\n",
    "dataset = dataset.normalizing(\n",
    "    sub_dir='normalized_quantile',\n",
    "    scalers = {\n",
    "        'node_attr' : ['quantile_transformer', 0],\n",
    "        # 'edge_attr' : ['minmax_scaler', 0],\n",
    "        'pressure' : ['quantile_transformer', None],\n",
    "        'flowrate' : ['quantile_transformer', None]\n",
    "    }\n",
    ")\n",
    "\n",
    "dataset = dataset.batching(\n",
    "    batch_size = args.batch_size,\n",
    "    batch_n_times = args.batch_n_time, \n",
    "    recursive = args.batch_recursive, \n",
    "    sub_dir='/normalized_quantile_batched', \n",
    "    step=args.batch_step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = OneDDatasetLoader(\n",
    "    root_dir='/data1/tam/downloaded_datasets_WT_v2',\n",
    "    sub_dir='normed_batched',\n",
    "    data_names='all',\n",
    "    time_names=[str(i).zfill(3) for i in range(201)]\n",
    ")\n",
    "\n",
    "train_loader, test_loader = dataset_split_to_loader(\n",
    "    dataset = dataset,\n",
    "    subset_ids = {\n",
    "        'train': list(range(0, 20)),\n",
    "        'test': list(range(30, 35))\n",
    "    },\n",
    "    n_datas_per_batch = args.n_datas_per_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare model\n",
    "model = RecurrentFormulationNetwork(\n",
    "    n_field=args.n_field,\n",
    "    n_meshfield=args.n_meshfield,\n",
    "    n_boundaryfield=args.n_boundaryfield,\n",
    "    # n_globalfield=args.n_globalfield,\n",
    "    n_hidden=args.n_hidden,\n",
    "    hidden_size=args.hidden_size,\n",
    "    integration=None\n",
    ")\n",
    "# model = torch_geometric.nn.DataParallel(model)\n",
    "model = model.to(args.device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "# args.setattr(attr_name='optimizer', attr_value=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, args, forward_sequence=False):\n",
    "    ## Field tensor: Tensor(n_node, n_time, n_field)\n",
    "    F_true = torch.cat([\n",
    "            data.pressure.unsqueeze(2),\n",
    "            data.flowrate.unsqueeze(2)\n",
    "        ], dim=2).float().to(args.device)\n",
    "    if not forward_sequence:\n",
    "        # forward only initial state\n",
    "        F = F_true[:,0,:]\n",
    "    else:\n",
    "        # forward all timestep\n",
    "        F = F_true\n",
    "    F_true = F_true[:,1:,:]\n",
    "    \n",
    "    ## Connectivity/edge_index: Tensor(2, n_edge)\n",
    "    # edge_index = data.edge_index.to(args.device)\n",
    "    edge_index = torch.cat([\n",
    "        data.edge_index, torch.flip(data.edge_index, dims=[0]\n",
    "    )], dim=1).to(args.device)\n",
    "\n",
    "    ## Mesh features: Tuple(Tensor(n_node, n_node_attr), Tensor(n_edge, n_edge_attr))\n",
    "    node_attr = data.node_attr.float().to(args.device)\n",
    "    # edge_attr = torch.cat([data.edge_attr, data.edge_attr], dim=0).float().to(args.device)\n",
    "    edge_attr = None\n",
    "    meshfield = (node_attr, edge_attr)\n",
    "\n",
    "    ## Boundary field\n",
    "    boundaryfield = torch.cat([data.flowrate[0,:].unsqueeze(0)]*data.flowrate.size(0), dim=0)\n",
    "    boundaryfield = boundaryfield.float().to(args.device)\n",
    "\n",
    "    # ## Global field\n",
    "    # globalfield = torch.cat([data.global_attr.unsqueeze(0)]*node_attr.size(0), dim=0)\n",
    "    # globalfield = globalfield.float().to(args.device)\n",
    "\n",
    "    ## Predict output sequence\n",
    "    F_pred = model(\n",
    "        F = F,\n",
    "        edge_index=edge_index,\n",
    "        meshfield=meshfield,\n",
    "        boundaryfield=boundaryfield,\n",
    "        # globalfield=globalfield,\n",
    "        forward_sequence=forward_sequence,\n",
    "        n_time=data.flowrate.size(1)\n",
    "    )\n",
    "    \n",
    "    ## loss calculation\n",
    "    # print(F_true.size(), F_pred.size())\n",
    "    loss = args.criterion(F_pred, F_true)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "# train(model, dataset[0], args, forward_sequence=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, data, args, forward_sequence=False):\n",
    "    ## Field tensor: Tensor(n_node, n_time, n_field)\n",
    "    F_true = torch.cat([\n",
    "            data.pressure.unsqueeze(2),\n",
    "            data.flowrate.unsqueeze(2)\n",
    "        ], dim=2).float().to(args.device)\n",
    "    if not forward_sequence:\n",
    "        # forward only initial state\n",
    "        F = F_true[:,0,:]\n",
    "    else:\n",
    "        # forward all timestep\n",
    "        F = F_true\n",
    "    F_true = F_true[:,1:,:]\n",
    "    \n",
    "    ## Connectivity/edge_index: Tensor(2, n_edge)\n",
    "    # edge_index = data.edge_index.to(args.device)\n",
    "    edge_index = torch.cat([\n",
    "        data.edge_index, torch.flip(data.edge_index, dims=[0]\n",
    "    )], dim=1).to(args.device)\n",
    "\n",
    "    ## Mesh features: Tuple(Tensor(n_node, n_node_attr), Tensor(n_edge, n_edge_attr))\n",
    "    node_attr = data.node_attr.float().to(args.device)\n",
    "    # edge_attr = data.edge_attr.float().to(args.device)\n",
    "    # edge_attr = torch.cat([data.edge_attr, data.edge_attr], dim=0).float().to(args.device)\n",
    "    edge_attr = None\n",
    "    meshfield = (node_attr, edge_attr)\n",
    "\n",
    "    ## Boundary field\n",
    "    boundaryfield = torch.cat([data.flowrate[0,:].unsqueeze(0)]*data.flowrate.size(0), dim=0)\n",
    "    boundaryfield = boundaryfield.float().to(args.device)\n",
    "\n",
    "    # ## Global field\n",
    "    # globalfield = torch.cat([data.global_attr.unsqueeze(0)]*node_attr.size(0), dim=0)\n",
    "    # globalfield = globalfield.float().to(args.device)\n",
    "\n",
    "    ## Predict output sequence\n",
    "    with torch.no_grad():\n",
    "        F_pred = model(\n",
    "            F = F,\n",
    "            edge_index=edge_index,\n",
    "            meshfield=meshfield,\n",
    "            boundaryfield=boundaryfield,\n",
    "            # globalfield=globalfield,\n",
    "            forward_sequence=forward_sequence,\n",
    "            n_time=data.flowrate.size(1)\n",
    "        )\n",
    "        \n",
    "        ## loss calculation\n",
    "        loss = args.criterion(F_pred, F_true)\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss = 0.00040585276997262554; eval loss = 0.9666224056854844\n",
      "Epoch 1: train loss = 0.00012802546424950378; eval loss = 0.1745601030997932\n",
      "Epoch 2: train loss = 3.555949609790332e-05; eval loss = 0.03760470013367012\n",
      "Epoch 3: train loss = 0.00011399848389714862; eval loss = 0.16122134460601956\n",
      "Epoch 4: train loss = 0.00011673552931767972; eval loss = 0.08398367225890979\n",
      "Epoch 5: train loss = 5.510589656765319e-05; eval loss = 0.01531207085645292\n",
      "Epoch 6: train loss = 2.3532425334593654e-05; eval loss = 0.024208789531257935\n",
      "Epoch 7: train loss = 2.267596809346628e-05; eval loss = 0.05833310403977521\n",
      "Epoch 8: train loss = 2.8815811919713497e-05; eval loss = 0.06409591823467053\n",
      "Epoch 9: train loss = 2.732691957874067e-05; eval loss = 0.035488810201059096\n",
      "Epoch 10: train loss = 2.110053697279568e-05; eval loss = 0.008698428922798485\n",
      "Epoch 11: train loss = 1.848051065422851e-05; eval loss = 0.00359968802149524\n",
      "Epoch 12: train loss = 2.032678924024367e-05; eval loss = 0.011250564944930375\n",
      "Epoch 13: train loss = 2.2283124394562037e-05; eval loss = 0.015141574978770223\n",
      "Epoch 14: train loss = 2.1423360408334702e-05; eval loss = 0.010547685669735074\n",
      "Epoch 15: train loss = 1.927988467187447e-05; eval loss = 0.004678733734181151\n",
      "Epoch 16: train loss = 1.827969962420184e-05; eval loss = 0.002407676693110261\n",
      "Epoch 17: train loss = 1.849461155956078e-05; eval loss = 0.0024064601711870637\n",
      "Epoch 18: train loss = 1.86499946295271e-05; eval loss = 0.003218804084099247\n",
      "Epoch 19: train loss = 1.8546974004607364e-05; eval loss = 0.006313192028756021\n",
      "Epoch 20: train loss = 1.8779019217873838e-05; eval loss = 0.010169736779062077\n",
      "Epoch 21: train loss = 1.9033157233394604e-05; eval loss = 0.010667260459740646\n",
      "Epoch 22: train loss = 1.900427737311361e-05; eval loss = 0.009043895654031076\n",
      "Epoch 23: train loss = 1.9033163987103308e-05; eval loss = 0.008450340257695643\n",
      "Epoch 24: train loss = 1.9136470982061837e-05; eval loss = 0.009504842812020797\n",
      "Epoch 25: train loss = 1.9242809194963684e-05; eval loss = 0.011130460901767947\n",
      "Epoch 26: train loss = 1.9350085228353464e-05; eval loss = 0.011759794171666726\n",
      "Epoch 27: train loss = 1.9413630539588667e-05; eval loss = 0.011529907649673987\n",
      "Epoch 28: train loss = 1.945608260811582e-05; eval loss = 0.011470696801552549\n",
      "Epoch 29: train loss = 1.949297260139815e-05; eval loss = 0.011664718898828141\n",
      "Epoch 30: train loss = 1.9518454600131463e-05; eval loss = 0.011680718802381307\n",
      "Epoch 31: train loss = 1.952649775205373e-05; eval loss = 0.011448415913037024\n",
      "Epoch 32: train loss = 1.9517812461344874e-05; eval loss = 0.01111161928565707\n",
      "Epoch 33: train loss = 1.949421391600481e-05; eval loss = 0.010707140521844849\n",
      "Epoch 34: train loss = 1.9456949623020137e-05; eval loss = 0.010226677099126391\n",
      "Epoch 35: train loss = 1.940742032502385e-05; eval loss = 0.009679916664026678\n",
      "Epoch 36: train loss = 1.9347161046567862e-05; eval loss = 0.009081096955924295\n",
      "Epoch 37: train loss = 1.9277437697695632e-05; eval loss = 0.008432985243416624\n",
      "Epoch 38: train loss = 1.919928135762916e-05; eval loss = 0.00774671946419403\n",
      "Epoch 39: train loss = 1.9113882792964887e-05; eval loss = 0.007044454843708081\n",
      "Epoch 40: train loss = 1.9023217621594313e-05; eval loss = 0.006349186762236059\n",
      "Epoch 41: train loss = 1.8930009670015124e-05; eval loss = 0.005676813241734635\n",
      "Epoch 42: train loss = 1.8836900629537467e-05; eval loss = 0.005027093036915176\n",
      "Epoch 43: train loss = 1.8745492031513322e-05; eval loss = 0.004401122630952159\n",
      "Epoch 44: train loss = 1.8656788761006737e-05; eval loss = 0.00380495020908711\n",
      "Epoch 45: train loss = 1.857167495700196e-05; eval loss = 0.003246843636588892\n",
      "Epoch 46: train loss = 1.8491225052486016e-05; eval loss = 0.0027351411790732527\n",
      "Epoch 47: train loss = 1.8416659465003704e-05; eval loss = 0.0022743494519090746\n",
      "Epoch 48: train loss = 1.834884538354231e-05; eval loss = 0.0018643704470377997\n",
      "Epoch 49: train loss = 1.828826403027506e-05; eval loss = 0.0015062487409522873\n",
      "Epoch 50: train loss = 1.823519960453268e-05; eval loss = 0.0011990531684205052\n",
      "Epoch 51: train loss = 1.8189721082251253e-05; eval loss = 0.0009425239768461324\n",
      "Epoch 52: train loss = 1.815174447017398e-05; eval loss = 0.0007352111188083654\n",
      "Epoch 53: train loss = 1.8121130541004504e-05; eval loss = 0.0005750476766479551\n",
      "Epoch 54: train loss = 1.8097672882078086e-05; eval loss = 0.00045885623467256664\n",
      "Epoch 55: train loss = 1.8081058076546697e-05; eval loss = 0.0003831150647783943\n",
      "Epoch 56: train loss = 1.8070693844407515e-05; eval loss = 0.000344030685710095\n",
      "Epoch 57: train loss = 1.806606639931374e-05; eval loss = 0.00033747994302757434\n",
      "Epoch 58: train loss = 1.8066571882968674e-05; eval loss = 0.00035904406422559987\n",
      "Epoch 59: train loss = 1.8071566795896388e-05; eval loss = 0.0004041067127218412\n",
      "Epoch 60: train loss = 1.808033874795001e-05; eval loss = 0.00046835923490107234\n",
      "Epoch 61: train loss = 1.8092173977635184e-05; eval loss = 0.0005470330879688845\n",
      "Epoch 62: train loss = 1.8106368401049622e-05; eval loss = 0.0006357399452099344\n",
      "Epoch 63: train loss = 1.812223008457181e-05; eval loss = 0.0007310262267310463\n",
      "Epoch 64: train loss = 1.8139088954427507e-05; eval loss = 0.0008302608530357247\n",
      "Epoch 65: train loss = 1.8156482916253935e-05; eval loss = 0.0009318374504800886\n",
      "Epoch 66: train loss = 1.8174094165601673e-05; eval loss = 0.0010333632058063813\n",
      "Epoch 67: train loss = 1.8191636680597867e-05; eval loss = 0.0011338318126945524\n",
      "Epoch 68: train loss = 1.820887378300995e-05; eval loss = 0.0012323735263635172\n",
      "Epoch 69: train loss = 1.8225700145535484e-05; eval loss = 0.0013266087817100924\n",
      "Epoch 70: train loss = 1.8241889591763538e-05; eval loss = 0.0014155033604765777\n",
      "Epoch 71: train loss = 1.8257262588861067e-05; eval loss = 0.0014981104050093563\n",
      "Epoch 72: train loss = 1.827164631862388e-05; eval loss = 0.0015743017975182738\n",
      "Epoch 73: train loss = 1.82848699665783e-05; eval loss = 0.0016422825374320382\n",
      "Epoch 74: train loss = 1.829681094989155e-05; eval loss = 0.0017015725952660432\n",
      "Epoch 75: train loss = 1.8307313226273436e-05; eval loss = 0.0017514051805846975\n",
      "Epoch 76: train loss = 1.831626699910771e-05; eval loss = 0.0017921454518727842\n",
      "Epoch 77: train loss = 1.832366889331638e-05; eval loss = 0.0018238667844343581\n",
      "Epoch 78: train loss = 1.8329499912539404e-05; eval loss = 0.0018469195174475317\n",
      "Epoch 79: train loss = 1.8333779152612806e-05; eval loss = 0.0018603432226882433\n",
      "Epoch 80: train loss = 1.833640255100022e-05; eval loss = 0.0018641338201632607\n",
      "Epoch 81: train loss = 1.8337337266416398e-05; eval loss = 0.0018591080925034476\n",
      "Epoch 82: train loss = 1.833662979677797e-05; eval loss = 0.0018441049724060576\n",
      "Epoch 83: train loss = 1.833417719154795e-05; eval loss = 0.001818670744796691\n",
      "Epoch 84: train loss = 1.8329992581556098e-05; eval loss = 0.0017852233095254633\n",
      "Epoch 85: train loss = 1.832430957549036e-05; eval loss = 0.0017465321625422803\n",
      "Epoch 86: train loss = 1.8317461684347336e-05; eval loss = 0.0017033811109286034\n",
      "Epoch 87: train loss = 1.830975615035868e-05; eval loss = 0.0016562691434955923\n",
      "Epoch 88: train loss = 1.8301339594017918e-05; eval loss = 0.0016053842828114284\n",
      "Epoch 89: train loss = 1.8292338161529642e-05; eval loss = 0.0015509641034441302\n",
      "Epoch 90: train loss = 1.828279324200821e-05; eval loss = 0.0014931407549738651\n",
      "Epoch 91: train loss = 1.82726880773032e-05; eval loss = 0.0014314080372059834\n",
      "Epoch 92: train loss = 1.8262054748419132e-05; eval loss = 0.0013664829657500377\n",
      "Epoch 93: train loss = 1.825100819274894e-05; eval loss = 0.0013010354596190155\n",
      "Epoch 94: train loss = 1.823978047710284e-05; eval loss = 0.0012364670456008753\n",
      "Epoch 95: train loss = 1.8228583069657134e-05; eval loss = 0.0011729060497600585\n",
      "Epoch 96: train loss = 1.821757144071512e-05; eval loss = 0.0011112057263744646\n",
      "Epoch 97: train loss = 1.820686228271029e-05; eval loss = 0.00105125863046851\n",
      "Epoch 98: train loss = 1.8196509937951078e-05; eval loss = 0.0009926926368279965\n",
      "Epoch 99: train loss = 1.8186536589581692e-05; eval loss = 0.0009365273531329876\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "total_train_loss = []\n",
    "total_eval_loss = []\n",
    "# batch = enumerate(list(range(0,10)))\n",
    "for epoch in range(args.epoch):\n",
    "    if epoch == 10:\n",
    "        args.forward_sequence = False\n",
    "    torch.cuda.empty_cache()\n",
    "    train_loss = 0\n",
    "    for i in range(train_loader.__len__()):\n",
    "        data = next(iter(train_loader))\n",
    "        # print(data)\n",
    "        train_loss += train(model=model, data=data, args=args, forward_sequence=True)\n",
    "\n",
    "    # train_loss /= train_loader.__len__() # len(train_dataset)\n",
    "    total_train_loss.append(train_loss)\n",
    "\n",
    "    eval_loss = 0\n",
    "    for i in range(test_loader.__len__()):\n",
    "        data = next(iter(test_loader))\n",
    "        eval_loss += eval(model=model, data=data, args=args)\n",
    "    # eval_loss /= test_loader.__len__() #len(eval_dataset)\n",
    "    total_eval_loss.append(eval_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch}: train loss = {train_loss}; eval loss = {eval_loss}')\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        torch.save(model.state_dict(), f'models/parc_test4_epoch{epoch+1}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for RecurrentFormulationNetwork:\n\tMissing key(s) in state_dict: \"pre_net.module_0.bias\", \"pre_net.module_0.lin.weight\", \"pre_net.module_1.weight\", \"pre_net.module_1.bias\", \"pre_net.module_3.bias\", \"pre_net.module_3.lin.weight\", \"net.down_convs.5.bias\", \"net.down_convs.5.lin.weight\", \"net.pools.4.weight\", \"net.up_convs.4.bias\", \"net.up_convs.4.lin.weight\", \"post_net.module_0.bias\", \"post_net.module_0.lin.weight\", \"post_net.module_2.bias\", \"post_net.module_2.lin.weight\", \"post_net.module_4.weight\", \"post_net.module_4.bias\", \"post_net.module_6.weight\", \"post_net.module_6.bias\", \"post_net.module_8.weight\", \"post_net.module_8.bias\". \n\tsize mismatch for net.down_convs.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for net.down_convs.0.lin.weight: copying a param with shape torch.Size([128, 22]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for net.down_convs.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for net.down_convs.1.lin.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for net.down_convs.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for net.down_convs.2.lin.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for net.down_convs.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for net.down_convs.3.lin.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for net.down_convs.4.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for net.down_convs.4.lin.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for net.pools.0.weight: copying a param with shape torch.Size([1, 128]) from checkpoint, the shape in current model is torch.Size([1, 256]).\n\tsize mismatch for net.pools.1.weight: copying a param with shape torch.Size([1, 128]) from checkpoint, the shape in current model is torch.Size([1, 256]).\n\tsize mismatch for net.pools.2.weight: copying a param with shape torch.Size([1, 128]) from checkpoint, the shape in current model is torch.Size([1, 256]).\n\tsize mismatch for net.pools.3.weight: copying a param with shape torch.Size([1, 128]) from checkpoint, the shape in current model is torch.Size([1, 256]).\n\tsize mismatch for net.up_convs.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for net.up_convs.0.lin.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for net.up_convs.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for net.up_convs.1.lin.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for net.up_convs.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for net.up_convs.2.lin.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for net.up_convs.3.bias: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for net.up_convs.3.lin.weight: copying a param with shape torch.Size([2, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 23\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# args.device = torch.device('cpu')\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# prepare model\u001b[39;00m\n\u001b[1;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m RecurrentFormulationNetwork(\n\u001b[1;32m     15\u001b[0m     n_field\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mn_field,\n\u001b[1;32m     16\u001b[0m     n_meshfield\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mn_meshfield,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     integration\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 23\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/parc_test3_epoch80.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     26\u001b[0m \u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m model\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(args\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     32\u001b[0m F_true \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\n\u001b[1;32m     33\u001b[0m         data\u001b[38;5;241m.\u001b[39mpressure\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     34\u001b[0m         data\u001b[38;5;241m.\u001b[39mflowrate\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     35\u001b[0m     ], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(args\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/anaconda3/envs/geometric/lib/python3.11/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for RecurrentFormulationNetwork:\n\tMissing key(s) in state_dict: \"pre_net.module_0.bias\", \"pre_net.module_0.lin.weight\", \"pre_net.module_1.weight\", \"pre_net.module_1.bias\", \"pre_net.module_3.bias\", \"pre_net.module_3.lin.weight\", \"net.down_convs.5.bias\", \"net.down_convs.5.lin.weight\", \"net.pools.4.weight\", \"net.up_convs.4.bias\", \"net.up_convs.4.lin.weight\", \"post_net.module_0.bias\", \"post_net.module_0.lin.weight\", \"post_net.module_2.bias\", \"post_net.module_2.lin.weight\", \"post_net.module_4.weight\", \"post_net.module_4.bias\", \"post_net.module_6.weight\", \"post_net.module_6.bias\", \"post_net.module_8.weight\", \"post_net.module_8.bias\". \n\tsize mismatch for net.down_convs.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for net.down_convs.0.lin.weight: copying a param with shape torch.Size([128, 22]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for net.down_convs.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for net.down_convs.1.lin.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for net.down_convs.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for net.down_convs.2.lin.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for net.down_convs.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for net.down_convs.3.lin.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for net.down_convs.4.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for net.down_convs.4.lin.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for net.pools.0.weight: copying a param with shape torch.Size([1, 128]) from checkpoint, the shape in current model is torch.Size([1, 256]).\n\tsize mismatch for net.pools.1.weight: copying a param with shape torch.Size([1, 128]) from checkpoint, the shape in current model is torch.Size([1, 256]).\n\tsize mismatch for net.pools.2.weight: copying a param with shape torch.Size([1, 128]) from checkpoint, the shape in current model is torch.Size([1, 256]).\n\tsize mismatch for net.pools.3.weight: copying a param with shape torch.Size([1, 128]) from checkpoint, the shape in current model is torch.Size([1, 256]).\n\tsize mismatch for net.up_convs.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for net.up_convs.0.lin.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for net.up_convs.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for net.up_convs.1.lin.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for net.up_convs.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for net.up_convs.2.lin.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for net.up_convs.3.bias: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for net.up_convs.3.lin.weight: copying a param with shape torch.Size([2, 256]) from checkpoint, the shape in current model is torch.Size([256, 256])."
     ]
    }
   ],
   "source": [
    "forward_sequence_validate = False\n",
    "# Load to evaluate\n",
    "dataset = OneDDatasetLoader(\n",
    "    root_dir='/data1/tam/downloaded_datasets_WT_v2',\n",
    "    sub_dir='normalized',\n",
    "    # sub_dir='normed_and_batched',\n",
    "    data_names='all',\n",
    "    time_names=[str(i).zfill(3) for i in range(201)]\n",
    ")\n",
    "\n",
    "data = dataset[38]\n",
    "# args.device = torch.device('cpu')\n",
    "# prepare model\n",
    "model = RecurrentFormulationNetwork(\n",
    "    n_field=args.n_field,\n",
    "    n_meshfield=args.n_meshfield,\n",
    "    n_boundaryfield=args.n_boundaryfield,\n",
    "    # n_globalfield=args.n_globalfield,\n",
    "    n_hidden=args.n_hidden,\n",
    "    hidden_size=args.hidden_size,\n",
    "    integration=None\n",
    ")\n",
    "model.load_state_dict(torch.load(\n",
    "    'models/parc_test3_epoch80.pth',\n",
    "    map_location='cuda:0'\n",
    "))\n",
    "model=model.to(args.device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "F_true = torch.cat([\n",
    "        data.pressure.unsqueeze(2),\n",
    "        data.flowrate.unsqueeze(2)\n",
    "    ], dim=2).float().to(args.device)\n",
    "if not forward_sequence_validate:\n",
    "    # forward only initial state\n",
    "    F = F_true[:,0,:]\n",
    "else:\n",
    "    # forward all timestep\n",
    "    F = F_true\n",
    "F_true = F_true[:,1:,:]\n",
    "\n",
    "## Connectivity/edge_index: Tensor(2, n_edge)\n",
    "# edge_index = data.edge_index.to(args.device)\n",
    "edge_index = torch.cat([\n",
    "    data.edge_index, torch.flip(data.edge_index, dims=[0]\n",
    ")], dim=1).to(args.device)\n",
    "\n",
    "## Mesh features: Tuple(Tensor(n_node, n_node_attr), Tensor(n_edge, n_edge_attr))\n",
    "node_attr = data.node_attr.float().to(args.device)\n",
    "# edge_attr = data.edge_attr.float().to(args.device)\n",
    "# edge_attr = torch.cat([data.edge_attr, data.edge_attr], dim=0).float().to(args.device)\n",
    "edge_attr = None\n",
    "meshfield = (node_attr, edge_attr)\n",
    "\n",
    "## Boundary field\n",
    "# boundaryfield = None\n",
    "boundaryfield = torch.cat([data.flowrate[0,:].unsqueeze(0)]*data.flowrate.size(0), dim=0)\n",
    "boundaryfield = boundaryfield.float().to(args.device)\n",
    "\n",
    "## Time tensor\n",
    "# time = data.time.float().to(args.device)\n",
    "time = None\n",
    "\n",
    "## Predict output sequence\n",
    "with torch.no_grad():\n",
    "    F_pred = model(\n",
    "        F = F,\n",
    "        edge_index=edge_index,\n",
    "        meshfield=meshfield,\n",
    "        boundaryfield=boundaryfield,\n",
    "        forward_sequence=forward_sequence_validate,\n",
    "        n_time=data.flowrate.size(1)\n",
    "    )\n",
    "\n",
    "node_list = [1, 100, 30000]\n",
    "## Draw pressure\n",
    "import matplotlib.pyplot as plt\n",
    "for i_node in node_list:\n",
    "    i_field = 0\n",
    "    y_pred = F_pred.cpu().numpy()[i_node,:,i_field]\n",
    "    y_true = F_true.cpu().numpy()[i_node,:,i_field]\n",
    "    # print(y_true, y_pred)\n",
    "    x = [i * 4.0 /200 for i in range(y_pred.shape[0])]\n",
    "    # print(data.node_attr.numpy()[i_node, 6])\n",
    "    # plt.ylim(-1,1)\n",
    "    plt.plot(x, y_pred, c='red', label='GNN Euler')\n",
    "    plt.plot(x, y_true, c='blue', linestyle='dashdot', label='ground_truth')\n",
    "    # plt.ylim([-1,1])\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Pressure', fontsize=20)\n",
    "    plt.xlabel('Time', fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "## Draw flowrate\n",
    "for i_node in node_list:\n",
    "    i_field = 1\n",
    "    y_pred = F_pred.cpu().numpy()[i_node,:,i_field]\n",
    "    y_true = F_true.cpu().numpy()[i_node,:,i_field]\n",
    "    x = [i * 4.0 /200 for i in range(y_pred.shape[0])]\n",
    "    # print(data.node_attr.numpy()[i_node, 6])\n",
    "    # plt.ylim(-1,1)\n",
    "    plt.plot(x, y_pred, c='red', label='GNN Euler')\n",
    "    plt.plot(x, y_true, c='blue', linestyle='dashdot', label='ground_truth')\n",
    "    # plt.ylim([-1,1])\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Flowrate', fontsize=20)\n",
    "    plt.xlabel('Time', fontsize=20)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
